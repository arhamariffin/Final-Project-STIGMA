# STAGE 1 - EXPLORATORY DATA ANALYSIS
(please hide this markdown to view stage 2)


### Import and Load Data

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import matplotlib as mpl
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
from scipy.stats import chi2_contingency
mpl.rcParams['font.family'] = 'sans-serif'
plt.style.use('ggplot')
sns.set()

# sklearn - preprocessing and modelling
from sklearn import preprocessing
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

from sklearn.model_selection import cross_validate
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import cross_validate
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import CategoricalNB
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import GradientBoostingClassifier
!pip install shap
import shap

# sns.set(rc={'figure.figsize':(20.7,8.27)})
# sns.set_style("whitegrid")
# sns.color_palette("dark")
# plt.style.use("fivethirtyeight")

!gdown 1Ug89rLFlYNGvZHKLORzQmMnZsdSXvgUn

df = pd.read_csv('aug_train.csv')

### Describe Data

#### Data Type

df.sample(5)

df.shape

Dataset terdiri 19158 baris, dan 14 Kolom(12 fitur, 1 target, 1 ID)

Apakah ada kolom dengan tipe data kurang sesuai, atau nama kolom dan isinya
kurang sesuai?


df.info()

Here we do some grouping from the candidate attributes<br>

Personal :
* gender
* enrolled_university

Education :
* education level
* major discipline

Geo :
* city
* city_development_index

Related to job :
* relevant_experience
* company_size
* company_type
* last_new_job
* experience

we aren't going to use 'training_hours' column after EDA because, training hours is counted as data leakage

**Columns that the Data Type should be change for analysis purpose** :
- enrollee_id column, the datatype need to be change to object because the ID column is not countable

- rename columns 'last_new_job' to 'last_job_tenure', 'enrolled_university' to 'current_enrolled_status' and 'relevent_experience' to 'relevant_experience'

- there's 1 not normal value in company size column, where all the values majority look like this '10-99',  there's one value is '10/49'

- the other columns is fine... even though we see there's some column that the data type should be change to int, we planned to binning the value of the columns later.

- the target column name should be change to 'JobChange', and the data type should be (object) Not Looking for job change and Looking for job change, not 1.0 and 0.0 (float)

dfs = df.copy()
dfs['enrollee_id'] = dfs['enrollee_id'].astype(str)
dfs.rename(columns={'last_new_job' : 'last_job_tenure',
                   'enrolled_university' : 'enrolled_status',
                   'relevent_experience' : 'relevant_experience'}, inplace=True)

dfs['company_size'].replace('10/49', '10-49', inplace=True)

dfs['target'] = dfs['target'].astype(int)
dfs['target'] = dfs['target'].astype(object)

dfs.columns

column name looks more fine now

#### Missing Values

B. Apakah ada kolom yang memiliki nilai kosong? Jika ada, apa saja?

dfs.isna().sum()

missing = dfs.isnull().sum()*100 / len(dfs)

percentage_missing = pd.DataFrame({'column':dfs.columns,
                                   'missing_percentage %':missing.values})
percentage_missing['missing_percentage %'] = percentage_missing['missing_percentage %'].round(2)
percentage_missing = percentage_missing.sort_values('missing_percentage %', ascending=False)
percentage_missing = percentage_missing.reset_index()
percentage_missing = percentage_missing.drop('index', axis=1)

# plot the missing value percentage
plt.figure(figsize=(10,5))
ax = sns.barplot(x='missing_percentage %', y='column', data=percentage_missing, color='#E1341E')
for p in ax.patches:
    ax.annotate("%.2f" % p.get_width() + '%', xy=(p.get_width(), p.get_y()+p.get_height()/2),
            xytext=(8, 0), textcoords='offset points' ,ha="left", va="center", fontsize=10)
plt.title('Missing values Percentage for Each Column', fontsize=17, fontweight='bold')
plt.ylabel('Kolom', fontsize=12)
plt.xlabel('missing_percentage %', fontsize=12)
plt.xlim(0,50)
plt.show()


**Kolom dengan nilai kosong** :
- ***company_type*** sebanyak 6140 ***(32.05%)***
- company_size sebanyak 5938 (30.99%)
- gender sebanyak 4508 (23.53%)
- major_discipline sebanyak 2813 (14.68%)
- education_level sebanyak 460 (2.4%)
- last_new_job sebanyak 423 (2.21%)
- enrolled_university sebanyak 386 (2.01%)
- experience sebanyak 65 (0.34%)


Kolom yang memiliki data kosong merupakan tipe data kategorik. Sehingga untuk tindak lanjut, akan dilakukan pengisian data-data yang kosong dengan nilai modus dari kategori masing-masing.



#### Statistic Summary

##### Kolom Numerik

nums = dfs[['city_development_index', 'training_hours']]
nums.describe().T

**Total count of individual data elements**

for col in nums:
    print(f'''Value Count column {col}:''')
    print(df[col].value_counts())
    print()

##### Kolom Kategorik

cats = dfs.select_dtypes(object)
cats.describe().T

**Total count of individual data elements**

for col in cats:
    print(f'''Value Count column {col}:''')
    print(dfs[col].value_counts())
    print()

- City dan experience terlalu banyak unique value
- data gender skewed karena male sebagai frekuensi terbanyak mencapai 90% dari jumlah count
- relevant experience tipe data diskrit. ada ketimpangan data karena yang has relevant experience mencakup lebih dari 70% keseluruhan data
- data data yang pada individual count kurang dari 1% akan dikategorikan lagi dan digabung dengan kategori lain untuk ditinjau ulang apakah dihapus atau tidak

## Visualisasi

### Univariate Analysis

#### Kolom target

plt.figure(figsize=(8,6))
sns.countplot(x='target', data=dfs, palette = ['#1ECBE1', '#E1341E'])
plt.grid(False)
plt.title('Target Count', fontsize=16, fontweight='bold')
plt.show()

# Assign value counts of target to target_counts variable
target_counts = dfs['target'].value_counts()

# Crate Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#F3F6FA')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(target_counts,
                                   explode=(0.08, 0),
                                   labels=['Not Looking For Job Change', 'Looking for Job Change'],
                                   colors=['#1ECBE1', '#E1341E'],
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({target_counts.iloc[i]})')

plt.text(-1.25, 1.15, "Target", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.75, 1.15, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.17, 1.15, "by", size=16, color="grey")
plt.text(0.38, 1.15, "Percentage", size=16, color="grey", fontweight="bold")

plt.show()

##### **Keterangan** :
- terdapat ketimpangan data kelas 75,1% : 24,9%
- untuk train model, kelas data akan diseimbangkan agar mesin tidak cenderung memprediksi kelas mayoritas saja

#### Distribusi Data Numerik



##### Histplot and Boxplot

feature_nums=['city_development_index','training_hours']
# Univariate analysis for numerical features
for feature in feature_nums:
    # Create a subplot for each numerical feature
    plt.figure(figsize=(14, 7))

    # Histogram for numerical features
    plt.subplot(1, 2, 1)
    sns.histplot(dfs[feature], kde=True)
    plt.title(f'Distribution of {feature}', size=16, fontweight='bold', pad=10)

    # Box plot for numerical features
    plt.subplot(1, 2, 2)
    sns.boxplot(y=dfs[feature])
    plt.title(f'Box Plot of {feature}', size=16, fontweight='bold', pad=10)

    plt.tight_layout()
    plt.show()

# Calculating Curtosis
city_dev_index_kurtosis = dfs['city_development_index'].kurtosis()
th_kurtosis = dfs['training_hours'].kurtosis()

# Menginterpretasikan nilai kurtosis
if city_dev_index_kurtosis > 0:
    interpretasi_kurtosis_c = "Leptokurtik (lebih berpuncak dan berekor berat)"
elif city_dev_index_kurtosis < 0:
    interpretasi_kurtosis_c = "Platykurtik (lebih datar dan ekor ringan)"
else:
    interpretasi_kurtosis_c = "Mesokurtik (mirip dengan distribusi normal)"

if th_kurtosis > 0:
    interpretasi_kurtosis = "Leptokurtik (lebih berpuncak dan berekor berat)"
elif th_kurtosis < 0:
    interpretasi_kurtosis = "Platykurtik (lebih datar dan ekor ringan)"
else:
    interpretasi_kurtosis = "Mesokurtik (mirip dengan distribusi normal)"

# Show the result
print("Kurtosis dari City Development index", city_dev_index_kurtosis)
print(f"Interpretasi: {interpretasi_kurtosis_c}")

print("Kurtosis dari Training Hours", th_kurtosis)
print(f"Interpretasi: {interpretasi_kurtosis}")

# Range: Difference between the maximum and minimum values
city_dev_index_range = dfs['city_development_index'].max() - dfs['city_development_index'].min()
th_range = dfs['training_hours'].max() - dfs['training_hours'].min()

# Variance: Measure of data point variability from the mean
city_dev_index_variance = dfs['city_development_index'].var()
th_variance = dfs['training_hours'].var()

# Standard Deviation: Average amount of deviation from the mean
city_dev_index_std_dev = dfs['city_development_index'].std()
th_std_dev = dfs['training_hours'].std()

# Interquartile Range (IQR): Spread between Q1 and Q3
city_dev_index_iqr = dfs['city_development_index'].quantile(0.75) - dfs['city_development_index'].quantile(0.25)
th_iqr = dfs['training_hours'].quantile(0.75) - dfs['training_hours'].quantile(0.25)

statistics_dfs_num = pd.DataFrame({
    'Statistik': ['Rentang', 'Varians', 'Deviasi Standar', 'Rentang Interkuartil (IQR)'],
    'City Development Index': [city_dev_index_range, city_dev_index_variance, city_dev_index_std_dev, city_dev_index_iqr],
    'Training Hours': [th_range, th_variance, th_std_dev, th_iqr]})

statistics_dfs_num

# Outliers
# Calculate the lower and upper fences
lower_fence = dfs['city_development_index'].quantile(0.25) - 1.5 * city_dev_index_iqr
upper_fence = dfs['city_development_index'].quantile(0.75)+ 1.5 * city_dev_index_iqr

count_low = len(dfs[dfs['city_development_index'] < lower_fence])
count_up = len(dfs[dfs['city_development_index'] > upper_fence])

# Display the count
print(f"Lower Whisker for city development index: {lower_fence:.2f}")
print(f"Upper Whisker for city development index: {upper_fence:.2f}")
print(f"Jumlah data yang termasuk outliers < {lower_fence}: {count_low}")
print(f"Jumlah data yang termasuk outliers > {upper_fence}: {count_up}")

# Outliers
# Calculate the lower and upper fences
lower_fence = dfs['training_hours'].quantile(0.25) - 1.5 *  th_iqr
upper_fence = dfs['training_hours'].quantile(0.75)+ 1.5 *  th_iqr

count_low = len(dfs[dfs['training_hours'] < lower_fence])
count_up = len(dfs[dfs['training_hours'] > upper_fence])

# Display the count
print(f"Lower Whisker for training hours: {lower_fence:.2f}")
print(f"Upper Whisker for training hours: {upper_fence:.2f}")
print(f"Jumlah data yang termasuk outliers < {lower_fence}: {count_low}")
print(f"Jumlah data yang termasuk outliers > {upper_fence}: {count_up}")

##### **Keterangan** :

#### Kolom city_development_index
- histogram berbentuk ***negatively skewed*** yaitu tipe distribusi yang nilai lebih banyak diplot di sisi kanan grafik, ekor distribusi lebih panjang di sisi kiri. Artinya frekuensi data lebih condong ke nilai yang lebih besar.
- mean lebih rendah daripada median dan modus. ***mean = 0.828848 < median = 0.903000 < modus = 0.920***
- kurtosis berjenis ***platykurtic*** yang artinya data pada kolom 'city_development_index' memiliki nilai ekstrim atau outlier yang lebih sedikit.
- dengan metode IQR, didapatkan batas bawah sebesar 0.47 dan batas atas sebesar 1.19. data yang termasuk kedalam outlier ada 17 data.


#### Kolom training_hours
- histogram berbentuk ***positively skewed*** yaitu tipe distribusi yang nilai lebih banyak diplot di sisi kiri grafik, ekor distribusi lebih panjang di sisi kanan. Artinya frekuensi data lebih condong ke nilai yang lebih kecil.
- mean lebih besar daripada median dan modus. ***mean = 65.366896 < median = 47 < modus = 28
- kurtosis berjenis ***Leptokurtic*** yang artinya data pada kolom 'training_hours' memiliki nilai atau outlier yang lebih ekstrim.
- dengan metode IQR, didapatkan batas bawah sebesar -74.5 dan batas atas sebesar 185.5. data yang termasuk kedalam outlier ada 984 data.


#### Distribusi Data Kategorikal

##### PIE CHART

# Gender
# Assign value counts of gender to gender_counts variable
gender_counts = dfs['gender'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(gender_counts,
                                   explode=[0.08] + [0.03]*(len(gender_counts)-1),
                                   labels=gender_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({gender_counts.iloc[i]})')

plt.text(-1.26, 1.4, "Gender", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.7, 1.4, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.17, 1.4, "by", size=16, color="grey")
plt.text(0.38, 1.4, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()

# Relevant Experience
# Assign value counts of relevant_experience to relevant_experience_counts variable
relevant_experience_counts = dfs['relevant_experience'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(relevant_experience_counts,
                                   explode=[0.08] + [0]*(len(relevant_experience_counts)-1),
                                   labels=relevant_experience_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({relevant_experience_counts.iloc[i]})')

plt.text(-1.8, 1.36, "Relevant Experience", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.36, 1.36, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.48, 1.36, "by", size=16, color="grey")
plt.text(0.69, 1.36, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()

# enrolled status
# Assign value counts of enrolled status to enrolled_status_counts variable
enrolled_status_counts = dfs['enrolled_status'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(enrolled_status_counts,
                                   explode=[0.08] + [0]*(len(enrolled_status_counts)-1),
                                   labels=enrolled_status_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({enrolled_status_counts.iloc[i]})')

plt.text(-1.46, 1.36, "Enrolled Status", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.36, 1.36, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.48, 1.36, "by", size=16, color="grey")
plt.text(0.69, 1.36, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()


# Education Level
# Assign value counts of education_level to education_level_counts variable
education_level_counts = dfs['education_level'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(education_level_counts,
                                   explode=[0.05] + [0]*(len(education_level_counts)-1),
                                   labels=education_level_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({education_level_counts.iloc[i]})')

plt.text(-1.5, 1.36, "Education Level", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.36, 1.36, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.48, 1.36, "by", size=16, color="grey")
plt.text(0.69, 1.36, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()


# Major Discipline
# Assign value counts of major_discipline to major_discipline_counts variable
major_discipline_counts = dfs['major_discipline'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(major_discipline_counts,

                                   labels=major_discipline_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({major_discipline_counts.iloc[i]})')

plt.text(-1.5, 1.36, "Major Discipline", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.36, 1.36, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.48, 1.36, "by", size=16, color="grey")
plt.text(0.69, 1.36, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()


# Company Type
# Assign value counts of company_type to company_type_counts variable
company_type_counts = dfs['company_type'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(company_type_counts,
                                   explode=[0.08] + [0]*(len(company_type_counts)-1),
                                   labels=company_type_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=30)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({company_type_counts.iloc[i]})')

plt.text(-1.45, 1.36, "Company Type", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.36, 1.36, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.48, 1.36, "by", size=16, color="grey")
plt.text(0.69, 1.36, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()

# Last Job Tenure
# Assign value counts of last_job_tenure to last_job_tenure_counts variable
last_job_tenure_counts = dfs['last_job_tenure'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(last_job_tenure_counts,
                                   explode=[0.035] + [0]*(len(last_job_tenure_counts)-1),
                                   labels=last_job_tenure_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({last_job_tenure_counts.iloc[i]})')

plt.text(-1.5, 1.36, "Last Job Tenure", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.36, 1.36, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.48, 1.36, "by", size=16, color="grey")
plt.text(0.69, 1.36, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()


##### BARPLOT

Kota mana sebagian besar enrollee berasal?

# Plot for 'city'
plt.figure(figsize=(20, 10))
data_city = dfs['city'].value_counts().head(10)
sns.barplot(y=data_city.index, x=data_city, palette="Set1")
plt.title('Top 10 Cities by Frequency', fontweight='bold',  size=14)
plt.xlabel('Count')
plt.ylabel('City')
plt.grid(False)
for i, v in enumerate(data_city):
    plt.text(v, i, str(v), ha='left', va='center', fontsize=11)
plt.show()

# Plot for the rest of categorical features
fig, axes = plt.subplots(3, 3, figsize=(20, 16))
axes = axes.ravel()  # Flatten the axes array

cats_feature=[ 'gender', 'relevant_experience', 'enrolled_status',
              'major_discipline', 'company_type',
               'experience', 'company_size', 'last_job_tenure', 'education_level']

# Define the order for each feature
orders = [
    None,  # order for 'gender'
    None,  # order for 'relevant_experience'
    None,  # order for 'enrolled_status'
    None,  # order for 'major_discipline'
    None,  # order for 'company_type'
    ['<1','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','>20'],  # order for 'experience'
    ['<10','10-49','50-99','100-500','500-999','1000-4999', '5000-9999', '10000+'],  # order for 'company_size'
    ['never','1','2','3','4','>4'],  # order for 'last_job_tenure'
    None,
]

for i, feature in enumerate(cats_feature):
    sns.countplot(data=dfs, x=feature, order=orders[i], palette='Set1', ax=axes[i])
    axes[i].set_title(f'Distribution of data in {feature}', size=14, fontweight='bold', pad=10)
    axes[i].grid(False)
    total_count = len(df)
    for p in axes[i].patches:
        height = p.get_height()
        axes[i].annotate(f'{height}', (p.get_x() + p.get_width() / 2., height), ha='center', va='bottom', size=10)

plt.tight_layout()
plt.show()

##### **Keterangan** :
Berdasarkan data perhitungan frekuensi di atas, data yang dianggap langka atau memiliki frekuensi sangat kecil (<2% setelah dikategorikan ulang) akan dihilangkan.
***kolom***
- 'gender' untuk kategori 'other' akan dihilangkan karena hanya 1% dari keseluruhan data
- 'education_level' akan dievaluasi lagi apakah datanya penting untuk kategori 'Phd' dan 'Primary School'
- 'major_discipline' valuenya akan diubah menjadi :
        * STEM = 'STEM'
        * Non STEM = 'humanities', 'business degree', 'arts', 'other'
        * No major = 'no_major'
- 'experience' akan dikelompokkan menjadi range
- 'company_size' akan dikelompokkan ulang agar lebih general.
- 'company_type' untuk kategori 'other akan dihapus karena valuenya sangat sedikit (121/1158)jadi tidak masalah untuk dihapus.
- 'last_job_tenure' akan dikategorikan ulang



Semua data yang telah digeneralisasi memiliki persentase yang >2% sehingga dapat dilakukan untuk analisis selanjutnya

### Multivariate Analysis (10 Poin)

####Kolom Kategorikal

Melihat korelasi antara kolom kategorikal dengan Target.



cats_feature=[ 'gender', 'relevant_experience', 'enrolled_status',
              'major_discipline', 'company_type', 'education_level',
               'experience', 'company_size', 'last_job_tenure']

# Define the order for each feature
orders = [
    None,  # order for 'gender'
    None,  # order for 'relevant_experience'
    None,  # order for 'enrolled_status'
    None,  # order for 'major_discipline'
    None,  # order for 'company_type'
    None,  # order for 'education_level'
    ['<1','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','>20'],  # order for 'experience'
    ['<10','10-49','50-99','100-500','500-999','1000-4999', '5000-9999', '10000+'],  # order for 'company_size'
    ['never','1','2','3','4','>4']  # order for 'last_job_tenure'
]

# Define the palette for the hue
palette = {0: '#1ECBE1', 1: '#E1341E'}

fig, axes = plt.subplots(3, 3, figsize=(30, 30))
axes = axes.ravel()  # Flatten the axes array

for i, feature in enumerate(cats_feature):
    if i >= len(axes):
        break
    countplot = sns.countplot(data=dfs, x=feature, order=orders[i], hue='target', palette=palette, ax=axes[i])
    axes[i].set_title(f'Distribution of data in {feature}', size=14, fontweight='bold', pad=10)
    axes[i].grid(False)
    total_count = len(df)
    for p in countplot.patches:
        height = p.get_height()
        axes[i].annotate(f'{height}', (p.get_x() + p.get_width() / 2., height), ha='center', va='bottom', size=10, weight='bold')

    # Change the legend labels
    legend = countplot.get_legend()
    legend.set_title('Target')
    for t, l in zip(legend.texts, ('Not Looking for Job Change', 'Looking for Job Change')):
        t.set_text(l)

plt.tight_layout()
plt.show()

# Memisahkan kolom berdasarkan tipe data
categorical_columns = ['gender', 'relevant_experience', 'enrolled_status', 'education_level', 'major_discipline', 'company_size', 'company_type', 'last_job_tenure']
target_column = 'target'

plt.figure(figsize=(16, 50))
for i, column in enumerate(categorical_columns):
    plt.subplot(10, 1, i+1)

    # Membuat crosstab antara kolom kategorikal dan kolom target
    crosstab = pd.crosstab(dfs[column], dfs[target_column])
    crosstab_percentage = crosstab.div(crosstab.sum(1), axis=0) * 100  # Menghitung persentase

    # Membuat stacked bar chart
    ax = sns.barplot(x=crosstab_percentage.index, y=crosstab_percentage[1], color='#E1341E', label='looking for a job change')
    ax = sns.barplot(x=crosstab_percentage.index, y=crosstab_percentage[0], color='#1ECBE1', label='not looking for a job change', bottom=crosstab_percentage[1])

    for c in ax.containers:
        labels = [str(round(v.get_height(), 2)) + "%" if v.get_height() > 0 else '' for v in c]
        ax.bar_label(c,
                 label_type='center',
                 labels = labels,
                 size = 14) # add a container object "c" as first argument
    # Remove spines
    for s in ["top", "right"]:
        ax.spines[s].set_visible(False)
    # Add labels
    ax.tick_params(labelsize = 14, labelrotation = 0)
    ax.set_ylabel("Percentage", size = 14)
    ax.set_xlabel(column, size = 14)
    # Add legend
    ax.legend(fontsize = 12,
          title = "Target",
          title_fontsize = 18,
          bbox_to_anchor = [0.55, 0.7])


    plt.xticks(rotation=0)
    plt.xlabel(column)
    plt.ylabel('Percentage')
    plt.title(f'Distribusi dari data {column} terhadap {target_column}', size=14, fontweight='bold', pad=10)
    plt.legend(title='Target', loc='upper right')
    plt.grid(False)



plt.tight_layout()
plt.show()

categoric = dfs.select_dtypes('object')
# Create an empty DataFrame to store the chi-square results
chi_square_results = pd.DataFrame(columns=['Feature', 'Chi-Square', 'P-Value'])

# Calculate chi-square for each categorical feature
for feature in categoric :
    contingency_table = pd.crosstab(dfs[feature], dfs['target'])
    chi2, p, _, _ = chi2_contingency(contingency_table)


    # Create a temporary DataFrame and concatenate it to the results DataFrame
    temp_df = pd.DataFrame({'Feature' : [feature], 'Chi-Square' : [chi2], 'P-Value' : [p]})
    chi_square_results = pd.concat([chi_square_results, temp_df], ignore_index=True)


# Save the results to a CSV file
chi_square_results.to_csv('chi_square_results.csv', index=False)

# Print the chi square result Ordered by strongest Correlation
chi_squaresult = pd.read_csv('chi_square_results.csv')
chi_squaresult = chi_squaresult.sort_values(by=['Chi-Square'],ascending=False)

# adding correlation column in chi square result
chi_squaresult['Correlation'] = chi_squaresult['P-Value'].apply(lambda p: 'Significant' if p < 0.05 else 'Not Significant')
chi_squaresult.to_csv('chi_square_results.csv', index=False)
chi_square_resultsn = pd.read_csv('chi_square_results.csv')
chi_square_resultsn

#### Kolom Numerikal

fig, axs = plt.subplots(1, 2, figsize=(15, 8))

# KDE plot for 'city_development_index' with 'target' distribution
sns.kdeplot(data=dfs[dfs['target'] == 1]['city_development_index'], label='Looking for a job change', shade=True, color='#E1341E', ax=axs[0])
sns.kdeplot(data=dfs[dfs['target'] == 0]['city_development_index'], label='Not looking for Job Change', shade=True, color='#1ECBE1', ax=axs[0])
axs[0].set_xlabel('City Development Index')
axs[0].set_ylabel('Density')
axs[0].set_title('KDE Plot for City Development Index by Target', fontweight='bold')
axs[0].grid(False)
axs[0].legend()

# KDE plot for 'training_hours' with 'target' distribution
sns.kdeplot(data=dfs[dfs['target'] == 1]['training_hours'], label='Looking for a job change', shade=True, color='#E1341E', ax=axs[1])
sns.kdeplot(data=dfs[dfs['target'] == 0]['training_hours'], label='Not looking for Job Change', shade=True, color='#1ECBE1', ax=axs[1])
axs[1].set_xlabel('Training Hours')
axs[1].set_ylabel('Density')
axs[1].set_title('KDE Plot for Training Hours by Target', fontweight='bold')
axs[1].grid(False)
axs[1].legend()

plt.tight_layout()
plt.show()

fig, axs = plt.subplots(1, 2, figsize=(15,8))

# Boxplot for 'city_development_index' with 'target' distribution
sns.boxplot(data=dfs, x='target', y='city_development_index', palette={0: '#1ECBE1', 1: '#E1341E'}, ax=axs[0])
axs[0].set_xlabel('Target')
axs[0].set_ylabel('City Development Index')
axs[0].set_title('Boxplot for City Development Index by Target', fontweight='bold')
axs[0].grid(False)
axs[0].legend()

# Boxplot for 'training_hours' with 'target' distribution
sns.boxplot(data=dfs, x='target', y='training_hours', palette={0: '#1ECBE1', 1: '#E1341E'}, ax=axs[1])
axs[1].set_xlabel('Target')
axs[1].set_ylabel('Training Hours')
axs[1].set_title('Boxplot for Training Hours by Target', fontweight='bold')
axs[1].grid(False)
axs[1].legend()

plt.tight_layout()
plt.grid(False)

plt.figure(figsize=(10, 6))
sns.boxplot(x='target', y='city_development_index', data=df, palette={0: '#1ECBE1', 1: '#E1341E'})
sns.despine()
plt.xlabel('Target')
plt.ylabel('City Development Index')
plt.title('Boxplot City Development Index by Target Distribution', fontweight='bold')
plt.grid(False)
plt.show()

# Buat boxplot untuk 'training_hours' dengan distribusi 'target' dengan warna yang diinginkan
plt.figure(figsize=(10, 6))
sns.boxplot(x='target', y='training_hours', data=df, palette={0: '#1ECBE1', 1: '#E1341E'})
sns.despine()
plt.xlabel('Target')
plt.ylabel('Training Hours')
plt.title('Boxplot Training Hours By Target Distribustion', fontweight='bold')
plt.grid(False)
plt.show()

#menyiapkan list baru untuk menampung nilai p-value & column
p_value_array, cols = [], []

# menghitung P-Value antara kolom numerikal dengan data target
for variable in nums[:-1]:
    groups = [dfs[dfs['target'] == target][variable] for target in dfs['target'].unique()]
    f_statistic, p_value =  stats.ttest_ind(*groups)

    p_value_array.append(p_value)
    cols.append(variable)

#masukan hasi p-value kedalam dataframe baru
uji_test = pd.DataFrame({'Cols':cols,
                         'P-Value':p_value_array})

uji_test['Keterangan'] = ['Terdapat perbedaan signifikan' if x < 0.05 else 'Tidak terdapat perbedaan signifikan' for x in uji_test['P-Value']]
uji_test

Analisis T-test

Kedua variabel prediktor yaitu city_development index dan training_hours berpengaruh secara signifikan terhadap peserta yang ingin 'job changes' atau 'not looking for job change'

uji_test2 = pd.DataFrame({'Target':[0.0, 1.0],
                          'Mean_city_development_index':[dfs[dfs['target']==0.0]['city_development_index'].mean(), dfs[dfs['target']==1.0]['city_development_index'].mean()],
                          'Mean_training_hours':[dfs[dfs['target']==0.0]['training_hours'].mean(), dfs[dfs['target']==1.0]['training_hours'].mean()]})

uji_test2['Target'] = uji_test2['Target'].map({0.0:'Not looking for job changes', 1.0:'looking for job changes'})

def uji(dfs, col1, col2, col3): #target, 'Mean_city_development_index', 'Mean_training_hours'
    fig, ax = plt.subplots(1, 2, figsize=(18, 8))

    sns.barplot(ax=ax[0], x=col1, y=col2, data=dfs, ci=None,  palette={'Not looking for job changes': '#1ECBE1', 'looking for job changes': '#E1341E'})
    for i in ax[0].containers:
        ax[0].bar_label(i,)
    ax[0].set_title('Mean_city_development_index vs Target')
    axs[0].grid(False)

    # multi vs realsum
    sns.barplot(ax=ax[1], x=col1, y=col3, data=dfs, ci=None,  palette={'Not looking for job changes': '#1ECBE1', 'looking for job changes': '#E1341E'})
    for i in ax[1].containers:
        ax[1].bar_label(i,)
    ax[1].set_title('Mean_training_hours vs Target')
    axs[1].grid(False)

uji(uji_test2, 'Target', 'Mean_city_development_index', 'Mean_training_hours')

1. peserta yang cenderung meninggalkan perusahaan setelah training memiliki karakter tempat tinggal di kota yang didominasi oleh city index yang rendah
2. peserta yang cenderung meninggalkan perusahaan setelah training memiliki karakter yang didominasi oleh training hours yang rendah

Korelasi antar kolom numerikal

# korelasi numerik menggunakan heat map
# Selecting numeric columns (excluding 'enrollee_id')
numeric_cols = df.select_dtypes(include=['int64', 'float64']).drop(columns=['enrollee_id'])

# Calculating the correlation between numeric columns
correlation_matrix = numeric_cols.corr()

# Creating a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Heatmap Korelasi antara Kolom Numerik (tanpa 'enrollee_id')")
plt.show()

Dari heatmap dapat dilihat bahwa city development index dengan hubungan terbalik terbesar. Selain itu, variabel lain yaitu Training_hour memiliki hubungan terbalik yang sama meski kekuatannya lemah.

Experience, city_development_index vs target

dft = dfs.copy()

# Define custom colors for 0 (Not Looking for Job Change) and 1 (Looking for Job Change)
custom_colors = {0: '#1ECBE1', 1: '#E1341E'}

# Define the order of X-axis labels for experience
experience_order = ['<1', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '>20']

# Set the order of X-axis labels for the 'experience' column
dft['experience'] = pd.Categorical(dft['experience'], categories=experience_order, ordered=True)

# Create a scatterplot with custom colors for target
plt.figure(figsize=(12, 6))
sns.scatterplot(data=dft, x='experience', y='city_development_index', hue='target', palette=custom_colors, size='target', sizes=(30, 200))

plt.title('Multivariate Analysis: Experience vs. City Development Index vs. Target', fontsize=14, fontweight='bold')
plt.xlabel('Experience')
plt.ylabel('City Development Index')
plt.legend(title='Target', labels=['Looking for Job Change', 'Not Looking for Job Change'])

plt.grid(False)
plt.tight_layout()
plt.show()


Insight:
Orang dengan pengalaman kerja lebih lama dan tinggal di kota dengan development index yang tinggi cenderung tidak mencari pekerjaan baru.

colors = {1.0: "#E1341E", 0.0: "#1ECBE1"}

# Plot bar dengan hue menggunakan kolom 'city_development_index' dan 'company_size_category' terhadap target
ax = sns.barplot(x='company_size', y='city_development_index', hue='target', data=dfs, palette=colors, ci= None)
# Menambahkan label pada sumbu-x
plt.xlabel('company_size_category')
# Menambahkan judul plot
plt.title('Hubungan antara City Development Index dan Company Size Category terhadap Target', size=14, fontweight='bold', pad=10)
# Mengganti label legenda
legend_labels = {1.0: 'looking for a job change', 0.0: 'no looking for a job change'}  # Sesuaikan dengan nilai target Anda
# Mendapatkan legenda dari plot
handles, _ = ax.get_legend_handles_labels()
# Mengatur ulang label legenda
for i, label in enumerate(handles):
    label.set_label(legend_labels[df['target'].unique()[i]])
# Menampilkan legenda
plt.legend(title='Target', title_fontsize=12, loc='upper right')
# Menampilkan plot
plt.grid(False)
plt.show()

Insight : <br>Pada barplot diatas, diketahui bahwa orang-orang yang berasal dari company_size_category small, large, maupun medium  yang memilih untuk **tidak mencari pekerjaan baru** di perusahaan setelah mengikuti training berada di kota dengan **index > 0.800** atau **Moderately Developed Cities**. Sedangkan, orang-orang yang **mencari pekerjaan baru** setelah mengikuti training berada di kota dengan **index < 0.800** atau **Less Developed Cities**.

dft['experience'] = pd.to_numeric(dft['experience'], errors='coerce')
colors = {1.0: "#E1341E", 0.0: "#1ECBE1"}

ax = sns.barplot(x='company_size', y='experience', hue='target', data=dft, palette=colors, ci=None)
plt.xlabel('company_size')
plt.title('Relationship between company_size_category and experience with respect to Target', size=14, fontweight='bold', pad=10)

legend_labels = {1.0: 'looking for a job change', 0.0: 'not looking for a job change'}

handles, _ = ax.get_legend_handles_labels()

for i, label in enumerate(handles):
    label.set_label(legend_labels[dft['target'].unique()[i]])

plt.legend(title='Target', title_fontsize=12, loc='upper left')
plt.grid(False)
plt.show()


Insight : <br>Pada barplot diatas, diketahui bahwa orang-orang yang berasal dari company_size_category small, large, maupun medium  yang memilih untuk **tidak mencari pekerjaan baru** di perusahaan setelah mengikuti training memiliki **pengalaman kerja > 10 tahun**. Sedangkan, orang-orang yang **mencari pekerjaan baru** setelah mengikuti training memiliki **pengalaman kerja < 8 tahun**, kecuali untuk **company_size_category large** memiliki **pengalaman kerja > 8 tahun**.


# Create a pivot table to count the number of people for each combination of gender and target
pivot_table_edu = dft.pivot_table(index='education_level', columns='target', values='enrollee_id', aggfunc='count', fill_value=0)

# Rename the columns for clarity
pivot_table_edu.columns = ['Not Looking for Job Change', 'Looking for Job Change']

# Reset the index to make 'gender' a regular column
pivot_table_edu.reset_index(inplace=True)


pivot_table_edu['Total Enrollees'] = pivot_table_edu['Not Looking for Job Change'] + pivot_table_edu['Looking for Job Change']

# Calculate the percentage of people looking for a new job from the total enrollees
pivot_table_edu['Percentage Looking for Job Change'] = (pivot_table_edu['Looking for Job Change'] / pivot_table_edu['Total Enrollees'] * 100).round(2)

pivot_table_edu = pivot_table_edu.sort_values('Percentage Looking for Job Change', ascending=False)
pivot_table_edu

# Define the order for sorting education levels
order = ["Phd", "Masters", "Graduate", "High School", "Primary School"]

pivot_table_edu = pivot_table_edu.set_index('education_level').loc[order].reset_index()

# Set the figure size
plt.figure(figsize=(12, 8))

# Define the colors for the bars
looking_for_job_change_color = "#E1341E"  # Red color
not_looking_for_job_change_color = "#1ECBE1"  # Blue color

# Create a grouped bar chart
bar_width = 0.4
positions = range(len(pivot_table_edu['education_level']))

plt.bar(positions, pivot_table_edu['Looking for Job Change'], bar_width, label='Looking for Job Change', color=looking_for_job_change_color)
plt.bar([pos + bar_width for pos in positions], pivot_table_edu['Not Looking for Job Change'], bar_width, label='Not Looking for Job Change', color=not_looking_for_job_change_color, alpha=0.5)

plt.xlabel('Education Level')
plt.ylabel('Number of People')
plt.title('Looking for Job Change vs. Not Looking for Job Change by Education Level', fontsize=14, fontweight='bold')
plt.xticks([pos + bar_width / 2 for pos in positions], pivot_table_edu['education_level'])

# Display the percentage values above the "Looking for Job Change" bar
# Display the total enrollee count between the bars
for i, row in pivot_table_edu.iterrows():
    plt.text(i + bar_width/2, max(row['Looking for Job Change'], row['Not Looking for Job Change']) + 50,
             f"Total: {row['Total Enrollees']}", ha='center', va='bottom', fontsize=10)
    plt.text(i + bar_width-0.40, row['Looking for Job Change'] + 30,
             f"{row['Percentage Looking for Job Change']}%", ha='center', va='bottom', fontsize=10, fontweight='bold')

plt.grid(False)

plt.legend()
plt.tight_layout()
plt.show()

Insight : <br>
Dari keseluruhan Karyawan yang diberikan training, Karyawan yang tingkat pendidikan Graduate lah yang paling tinggi yaitu 11598 dari 19158 karyawan, dan juga yang paling banyak tetap stay di perusahaan **(27.98% dari 11958 karyawan graduate Churn)**. Karyawan tingkat pendidikan Primary School yang paling rendah dari keseluruhan karyawan yang diberikan training ini(308 Karyawan).

# Stage 2 - Preprocessing
(please hide this markdown to view stage 3)

## 1. Data Cleansing

### A. Handle Missing Value

#creates a copy of dfs dataframe for preprocessing
dfk = dfs.copy()

# Checking missing values
dfk.isna().sum()

Handling Missing Values with Imputation Method

# Values to fill in missing data
imputer = {'gender' : 'Female',
           'enrolled_status' : 'Full time course',
           'education_level' : 'High School',
           'major_discipline' : 'Business Degree',
           'experience' : '15',
           'company_type' : 'Funded Startup',
           'company_size' : '1000-4999' ,
           'last_job_tenure' : '2'}

# fill missing value with values from the imputer dictionary.
dfk.fillna(value=imputer, inplace=True)

dfk.isna().sum()

Missing value sudah teratasi

### B. Handle Duplicated Data

#checking duplicated data for overall dataset
dfk.duplicated().sum()

dfk['enrollee_id'].duplicated().sum()

'enrollee_id' merupakan feature yang menjadi penanda atau dapat disebut dengan primary key. dengan mengecek kemungkinan duplikasi yang ada pada kolom 'enrollee_id' dapat memastikan ada tidaknya data yang redundant.

### C. Feature Transformation<br>
Mengelompokkan ulang agar kategori menjadi lebih general dan mengurangi unique value.

education_level : Highschool and primary school = non-degree holder

# Drop 'other' value on gender columns
dfk = dfk[dfk['gender'] != 'Other']
dfk = dfk[dfk['company_type'] != 'Other']

# city_development_index
conditions_city = [
    (dfk['city_development_index'] > 0.8),
    (dfk['city_development_index'] >= 0.6) & (dfk['city_development_index'] < 0.8),
    (dfk['city_development_index'] < 0.6)
]

choices_city = ['Highly Developed Cities', 'Moderately Developed Cities', 'Less Developed Cities']
dfk['city_category'] = np.select(conditions_city, choices_city, default='Unknown')

# last_job_tenure
conditions_last_job = [
    (dfk['last_job_tenure'] == '>4'),
    (dfk['last_job_tenure'] >= '2') & (dfk['last_job_tenure'] <= '4'),
    (dfk['last_job_tenure'].isin(['1', '<1']))
]

choices_last_job = ['High Tenure', 'Medium Tenure', 'Low Tenure']
dfk['job_tenure_category'] = np.select(conditions_last_job,
                                         choices_last_job,
                                         default='Never')

# experience
def categorize_experience(experience):
    if experience in ['10', '11', '12', '13', '14',
                      '15', '16', '17', '18', '19', '20', '>20']:
        return 'Senior Level'
    elif experience in ['4', '5', '6', '7', '8', '9']:
        return 'Intermediate Level'
    elif experience in ['<1', '1', '2', '3']:
        return 'Junior Level'
    return 'Unknown'

dfk['experience_category'] = dfk['experience'].apply(categorize_experience)

# company_size
conditions_company_size = [
    (dfk['company_size'].isin(['<10', '10-49', '50-99'])),
    (dfk['company_size'].isin(['100-500', '500-999'])),
    (dfk['company_size'].isin(['1000-4999', '5000-9999', '10000+']))
]

choices_company_size = ['Small', 'Medium', 'Large']
dfk['company_size_category'] = np.select(conditions_company_size, choices_company_size, default='Unknown')


# education level (fixed)
def edu(level):
    if level in ['Primary School', 'High School']:
        return 'Non Degree Holder'
    elif level in 'Masters':
        return 'Masters'
    elif level in 'Graduate':
        return 'Graduate'
    else:
        return 'Phd'

dfk['edulevel'] = dfk['education_level'].apply(edu)

dfk.columns

new_cat_feature = ['city_category','job_tenure_category',
                   'experience_category', 'company_size_category', 'edulevel']
for col in new_cat_feature:
    print(f'''Value Count column {col}:''')
    print(dfk[col].value_counts())
    print()

## Visual after Data Cleansing

### Pie Chart Each Feature

# Gender
# Assign value counts of gender to gender_counts variable
gender_counts = dfk['gender'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(gender_counts,
                                   explode=[0.04] + [0.03]*(len(gender_counts)-1),
                                   labels=gender_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({gender_counts.iloc[i]})')

plt.text(-1.26, 1.4, "Gender", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.7, 1.4, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.17, 1.4, "by", size=16, color="grey")
plt.text(0.38, 1.4, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()

# Relevant Experience
# Assign value counts of relevant_experience to relevant_experience_counts variable
relevant_experience_counts = dfk['relevant_experience'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(relevant_experience_counts,
                                   explode=[0.04] + [0]*(len(relevant_experience_counts)-1),
                                   labels=relevant_experience_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({relevant_experience_counts.iloc[i]})')

plt.text(-1.8, 1.36, "Relevant Experience", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.36, 1.36, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.48, 1.36, "by", size=16, color="grey")
plt.text(0.69, 1.36, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()

# enrolled_status
# Assign value counts of enrolled status to enrolled_status_counts variable
enrolled_status_counts = dfk['enrolled_status'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(enrolled_status_counts,
                                   explode=[0.04] + [0]*(len(enrolled_status_counts)-1),
                                   labels=enrolled_status_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({enrolled_status_counts.iloc[i]})')

plt.text(-1.46, 1.36, "Enrolled Status", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.36, 1.36, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.48, 1.36, "by", size=16, color="grey")
plt.text(0.69, 1.36, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()

# Education Level
# Assign value counts of education_level to education_level_counts variable
education_level_counts = dfk['edulevel'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(education_level_counts,
                                   explode=[0.04] + [0]*(len(education_level_counts)-1),
                                   labels=education_level_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({education_level_counts.iloc[i]})')

plt.text(-1.48, 1.36, "Education Level", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.36, 1.36, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.48, 1.36, "by", size=16, color="grey")
plt.text(0.69, 1.36, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()


# Company Type
# Assign value counts of company_type to company_type_counts variable
company_type_counts = dfk['company_type'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(company_type_counts,
                                   explode=[0.04] + [0]*(len(company_type_counts)-1),
                                   labels=company_type_counts.index,
                                   autopct='%1.1f%%',
                                   pctdistance=0.85,
                                   labeldistance=1.1,
                                   shadow=True,
                                   startangle=30)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({company_type_counts.iloc[i]})')

plt.text(-1.38, 1.36, "Company Type", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.36, 1.36, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.48, 1.36, "by", size=16, color="grey")
plt.text(0.69, 1.36, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()

# City Category
# Assign value counts of gender to gender_counts variable
city_counts = dfk['city_category'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(city_counts,
                                   explode=[0.04] + [0.0]*(len(city_counts)-1),
                                   labels=city_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({city_counts.iloc[i]})')

plt.text(-1.7, 1.4, "City Category", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.7, 1.4, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.17, 1.4, "by", size=16, color="grey")
plt.text(0.38, 1.4, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()


# Job Tenure Category
# Assign value counts of job_tenure_category to job_counts variable
job_counts = dfk['job_tenure_category'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(job_counts,
                                   explode=[0.04] + [0.0]*(len(job_counts)-1),
                                   labels=job_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({job_counts.iloc[i]})')

plt.text(-1.48, 1.4, "Job Tenure", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.7, 1.4, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.17, 1.4, "by", size=16, color="grey")
plt.text(0.38, 1.4, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()


# Experience Category
# Assign value counts of Experience Category to exp_counts variable
exp_counts = dfk['experience_category'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(exp_counts,
                                   explode=[0.04] + [0.0]*(len(exp_counts)-1),
                                   labels=exp_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({exp_counts.iloc[i]})')

plt.text(-1.48, 1.4, "Experience", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.7, 1.4, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.17, 1.4, "by", size=16, color="grey")
plt.text(0.38, 1.4, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()


# Company Size Category
# Assign value counts of company_size_category to company_counts variable
company_counts = dfk['company_size_category'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(company_counts,
                                   explode=[0.04] + [0.0]*(len(company_counts)-1),
                                   labels=company_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({company_counts.iloc[i]})')

plt.text(-1.7, 1.4, "Company Size", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.7, 1.4, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.17, 1.4, "by", size=16, color="grey")
plt.text(0.38, 1.4, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()


# major Category
# Assign value counts of major to major_counts variable
major_counts = dfk['major_discipline'].value_counts()

# Create Pie Chart
fig, ax = plt.subplots(figsize=(10, 5))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(major_counts,
                                   explode=[0.04] + [0.0]*(len(major_counts)-1),
                                   labels=major_counts.index,
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Show text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({major_counts.iloc[i]})')

plt.text(-1.4, 1.4, "Major Discipline", size=16, color='#004AAD', fontweight="bold")
plt.text(-0.7, 1.4, "Distribution", size=16, color="grey", fontweight="bold")
plt.text(0.17, 1.4, "by", size=16, color="grey")
plt.text(0.38, 1.4, "Percentage", size=16, color="grey", fontweight="bold")

plt.tight_layout()
plt.show()

### Barplot Each Feature

# Plot for the rest of categorical features
fig, axes = plt.subplots(5, 2, figsize=(17, 17))
axes = axes.ravel()  # Flatten the axes array

cats_feature=['gender', 'relevant_experience', 'enrolled_status',
               'company_type', 'city_category', 'job_tenure_category',
               'experience_category', 'company_size_category', 'major_discipline', 'edulevel']

for i, feature in enumerate(cats_feature):
    sns.countplot(data=dfk, x=feature, order=dfk[feature].value_counts().index, palette='Set1', ax=axes[i])
    axes[i].set_title(f'Distribution of {feature}', size=14, fontweight='bold', pad=10)
    axes[i].grid(False)
    total_count = len(dfk)
    for p in axes[i].patches:
        height = p.get_height()
        axes[i].annotate(f'{height}', (p.get_x() + p.get_width() / 2., height), ha='center', va='bottom', size=10)

plt.tight_layout()
plt.show()

# Define the palette for the hue
palette = {0: '#1ECBE1', 1: '#E1341E'}

fig, axes = plt.subplots(5, 2, figsize=(17, 19))
axes = axes.ravel()  # Flatten the axes array

cats_feature=['gender', 'relevant_experience', 'enrolled_status', 'edulevel',
               'company_type', 'city_category', 'job_tenure_category',
               'experience_category', 'company_size_category', 'major_discipline']

for i, feature in enumerate(cats_feature):
    countplot = sns.countplot(data=dfk, x=feature, order=dfk[feature].value_counts().index,hue='target', palette=palette, ax=axes[i])
    axes[i].set_title(f'Distribution of data in {feature}', size=14, pad=10, fontweight='bold')
    axes[i].grid(False)
    total_count = len(dfk)
    for p in axes[i].patches:
        height = p.get_height()
        axes[i].annotate(f'{height}', (p.get_x() + p.get_width() / 2., height), ha='center', va='bottom', size=8.5)

    # Change the legend labels
    legend = countplot.get_legend()
    legend.set_title('Target')
    for t, l in zip(legend.texts, ('Not Looking for Job Change', 'Looking for Job Change')):
        t.set_text(l)

plt.tight_layout()
plt.show()

# Define the palette for the hue
palette = {0: '#1ECBE1', 1: '#E1341E'}

fig, axes = plt.subplots(5, 2, figsize=(26, 20))
axes = axes.ravel()  # Flatten the axes array

cats_feature=['gender', 'relevant_experience', 'enrolled_status', 'edulevel',
               'company_type', 'city_category', 'job_tenure_category',
               'experience_category', 'company_size_category', 'major_discipline']

for i, feature in enumerate(cats_feature):
    countplot = sns.countplot(data=dfk, x=feature, order=dfk[feature].value_counts().index,hue='target', palette=palette, ax=axes[i])
    axes[i].set_title(f'Distribution of data in {feature}', size=14, pad=10)
    axes[i].grid(False)
    total_count = len(dfk)
    for p in axes[i].patches:
        height = p.get_height()
        axes[i].annotate(f'{height}', (p.get_x() + p.get_width() / 2., height), ha='center', va='bottom', size=10)

    # Change the legend labels
    legend = countplot.get_legend()
    legend.set_title('Target')
    for t, l in zip(legend.texts, ('Not Looking for Job Change', 'Looking for Job Change')):
        t.set_text(l)

plt.tight_layout()
plt.show()

## Feature Engineering

### D. Feature Selection

sebelum memilih fitur disini kami melihat sekali lagi korelasi antara fitur kategorikal dengan target menggunakan Chi Square

categoric = dfk.select_dtypes('object')
# Create an empty DataFrame to store the chi-square results
chi_square_results = pd.DataFrame(columns=['Feature', 'Chi-Square', 'P-Value'])

# Calculate chi-square for each categorical feature
for feature in categoric :
    contingency_table = pd.crosstab(dfk[feature], dfk['target'])
    chi2, p, _, _ = chi2_contingency(contingency_table)


    # Create a temporary DataFrame and concatenate it to the results DataFrame
    temp_df = pd.DataFrame({'Feature' : [feature], 'Chi-Square' : [chi2], 'P-Value' : [p]})
    chi_square_results = pd.concat([chi_square_results, temp_df], ignore_index=True)


# Save the results to a CSV file
chi_square_results.to_csv('chi_square_results.csv', index=False)

# Print the chi square result Ordered by strongest Correlation
chi_squaresult = pd.read_csv('chi_square_results.csv')
chi_squaresult = chi_squaresult.sort_values(by=['P-Value'],ascending=True)

# adding correlation column in chi square result
chi_squaresult['Correlation'] = chi_squaresult['P-Value'].apply(lambda p: 'Significant' if p < 0.05 else 'Not Significant')
chi_squaresult.to_csv('chi_square_results.csv', index=False)
chi_square_resultsn = pd.read_csv('chi_square_results.csv')
chi_square_resultsn

dfk.columns

dfc = dfk[[
      #  'enrollee_id', 'city', 'city_development_index','gender',
      'relevant_experience', 'enrolled_status',
      # 'education_level', 'major_discipline',
      #  'experience', 'company_size',
      'company_type',
      # 'last_job_tenure', 'training_hours',
      'target', 'city_category','job_tenure_category',
      'experience_category', 'company_size_category',
       'edulevel'
       ]].copy()

dfc.sample(3)

### E. Feature engineering - Feature Extraction
Tidak dilakukan karena semua feature berbentuk kategorik

### F. Feature engineering - Feature Encoding


# Print all the feature value counts to Encode the feature Easier
for col in dfc:
    print(f'''Value Count column {col}:''')
    print(dfc[col].value_counts())
    print()

#### Label Encoding

Label Encode all features : <br>



# encoded relevant_experience
map_rx = {'Has relevent experience' : 0,
          'No relevent experience' : 1}
dfc['relevant_experience'] = dfc['relevant_experience'].map(map_rx)

# encoded enrolled Status
map_en = {'no_enrollment' : 0,
          'Full time course' : 1,
          'Part time course' : 2}
dfc['enrolled_status'] = dfc['enrolled_status'].map(map_en)

#encoded feature education_level
mapping_edu = {'Graduate' : 1,
               'Masters' : 2,
               'Non Degree Holder' : 0,
               'Phd' : 3
               }
dfc['edulevel'] = dfc['edulevel'].map(mapping_edu)

#encoded feature city_category
mapping_city_cat = {'Highly Developed Cities':2,
                    'Moderately Developed Cities':1,
                    'Less Developed Cities':0
                    }
dfc['city_category'] = dfc['city_category'].map(mapping_city_cat)

# encoded feature company_size_category
mapping_csize = {'Large':2,
                 'Small':0,
                 'Medium':1
                 }
dfc['company_size_category'] = dfc['company_size_category'].map(mapping_csize)

# encoded feature job_tenure_category
mapping_new_job = {'Low Tenure' : 1,
                   'Medium Tenure' : 2,
                   'High Tenure' : 3,
                   'Never' : 0}
dfc['job_tenure_category'] = dfc['job_tenure_category'].map(mapping_new_job)

# encode company type based on their funding
mapping_company = {'Pvt Ltd' : 0,
                   'Funded Startup' : 1,
                   'Public Sector' : 2,
                   'Early Stage Startup' : 3,
                   'NGO' : 4,
                   'Other' : 5}
dfc['company_type'] = dfc['company_type'].map(mapping_company)

# encoed feature experience_category
map_ex = {'Senior Level' : 2,
          'Intermediate Level' : 1,
          'Junior Level' : 0}
dfc['experience_category'] = dfc['experience_category'].map(map_ex)

for col in dfc.columns:
    value_counts = dfc[col].value_counts()
    print(f"Value Counts for {col}:\n{value_counts}\n")

dfc.sample(3)

### G. Handling Imbalanced Data

#### SMOTE Over Sampling

# define x and y
x = dfc.drop(columns='target')
y = dfc['target'].astype(int)

#split x y train and x y test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=17)

# Initialize SMOTE
smote = SMOTE(random_state=17)

# Doing oversampling to Data Train
x_over, y_over = smote.fit_resample(x_train, y_train)

# Set a custom color palette for the countplot
bright_palette = sns.color_palette("muted")

# Create the countplot with the custom color palette
plt.figure(figsize=(7, 5))
ax = sns.countplot(x=y_over, data=x_over, palette=bright_palette)
sns.despine()

# Add annotations to the bars
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height}', (p.get_x() + p.get_width() / 2., height),
                ha='center', va='bottom', size=12)

plt.grid(False)
plt.show()

target_counts = y_over.value_counts()

# Create a pie chart
fig, ax = plt.subplots(figsize=(6, 3))
fig.patch.set_facecolor('#e6ebed')

plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.weight'] = 'bold'

wedges, texts, autotexts = ax.pie(target_counts,
                                   explode=(0.05, 0),
                                   labels=['Not Looking for Job Change', 'Looking for Job Change'],
                                   colors=['#1ECBE1', '#E1341E'],
                                   autopct='%1.1f%%',
                                   shadow=True,
                                   startangle=80)

plt.axis('equal')

# Display the total count of each category as text
for i, text in enumerate(texts):
    text.set(text=texts[i].get_text() + f'\n({target_counts.iloc[i]})')

plt.title('Data Target Sesudah Handling Imbalance Data Dengan SMOTE', fontsize=13, fontweight='bold')

plt.show()

# Stage 3 Modelling

x_train = x_over
y_train = y_over

def eval_classification(model) :
  y_pred = model.predict(x_test)
  y_pred_train = model.predict(x_train)
  y_pred_proba = model.predict_proba(x_test)
  y_pred_proba_train = model.predict_proba(x_train)

  print('Confusion matrix data train: ')
  print(confusion_matrix(y_train, y_pred_train))
  print('          ')
  print('Confusion matrix data test: ')
  print(confusion_matrix(y_test, y_pred))
  print('          ')

  print('Accuracy (Train Set): %.2f' % accuracy_score(y_train, y_pred_train))
  print('Accuracy (Test Set): %.2f' % accuracy_score(y_test, y_pred))
  print('Precision (Train Set): %.2f' % precision_score(y_train, y_pred_train))
  print('Precision (Test Set): %.2f' % precision_score(y_test, y_pred))
  print('Recall (Train Set): %.2f' % recall_score(y_train, y_pred_train))
  print('Recall (Test Set): %.2f' % recall_score(y_test, y_pred))
  print('F1-Score (Train Set): %.2f' % f1_score(y_train, y_pred_train))
  print('F1-Score (Test Set): %.2f' % f1_score(y_test, y_pred))

  print('AUC (train-proba): %.2f' % roc_auc_score(y_train, y_pred_proba_train[:, 1]))
  print('AUC (test-proba): %.2f' % roc_auc_score(y_test, y_pred_proba[:, 1]))

  score = cross_validate(model, x, y, cv=5, scoring='roc_auc', return_train_score=True)
  print('ROC-AUC (crossval train):' + str(score['train_score']))
  print('ROC-AUC (crossval train):' + str(score['train_score'].mean()))
  print('ROC-AUC (crossval test):' + str(score['test_score']))
  print('ROC-AUC (crossval test):' + str(score['test_score'].mean()))

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=x.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model):
    print(model.best_estimator_.get_params())

## Fit Model

### Decision Tree

dt = DecisionTreeClassifier()
dt.fit(x_train, y_train)
eval_classification(dt)

### Random Forest

rf = RandomForestClassifier()
rf.fit(x_train, y_train)
eval_classification(rf)

### AdaBoost

ab = AdaBoostClassifier()
ab.fit(x_train, y_train)
eval_classification(ab)

### XGBoost

xgb = XGBClassifier(random_state=17)
xgb.fit(x_train, y_train)
eval_classification(xgb)

### GradientBoosting

gb = GradientBoostingClassifier(random_state=17)
gb.fit(x_train, y_train)
eval_classification(gb)

# Assuming X_test is your test dataset and model is your Gradient Boosting model
y_pred = gb.predict(x_test)

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, y_pred)

# Calculate the percentage of each category
percentage_matrix = cnf_matrix / cnf_matrix.sum()

# Define the labels
labels = np.array([['True Negative', 'False Positive'],
                   ['False Negative', 'True Positive']])

# Create labels with both count and percentage
new_labels = (np.asarray(["{0}\n{1} ({2:.2%})".format(label, value, percentage)
                          for label, value, percentage in zip(labels.flatten(), cnf_matrix.flatten(), percentage_matrix.flatten())])
             ).reshape(2,2)

# Plot confusion matrix using a heatmap
fig = plt.figure(figsize=(10,7))
fig.patch.set_facecolor('white')  # Change figure color

sns.heatmap(cnf_matrix, annot=new_labels, fmt='', cmap='viridis', annot_kws={"weight": "bold"})  # Change heatmap color to 'viridis'
plt.xlabel('Predicted')
plt.ylabel('Actual')

# Change label colors
plt.gca().xaxis.label.set_color('black')
plt.gca().yaxis.label.set_color('black')

plt.show()

## Hyperparameter tuning Gradient Boosting

# List of hyperparameters
learning_rate = [float(x) for x in np.linspace(0.01, 0.04, 50)]
n_estimators = [int(x) for x in np.linspace(1, 100, 50)]
min_samples_split = [int(x) for x in np.linspace(2, 10, 10)]
min_samples_leaf = [int(x) for x in np.linspace(1, 10, 10)]
max_depth = [int(x) for x in np.linspace(1, 10, 10)]
max_leaf_nodes = [int(x) for x in np.linspace(1,10,10)]
n_iter_no_change = [int(x) for x in np.linspace(1,10,10)]

hyperparameters = dict(learning_rate=learning_rate, n_estimators=n_estimators,
                       min_samples_leaf=min_samples_leaf,min_samples_split=min_samples_split,
                       max_depth=max_depth,max_leaf_nodes=max_leaf_nodes, n_iter_no_change=n_iter_no_change)

# Crossvalidate with GridSearch
gb = GradientBoostingClassifier()
stratified_kfold = StratifiedKFold(n_splits = 5, shuffle=True, random_state=17)
gb_tune = RandomizedSearchCV(gb, hyperparameters, scoring='recall', cv=stratified_kfold)
gb_tune.fit(x_train, y_train)
eval_classification(gb_tune)

### Learning Curve

#### learning rate

def draw_learning_curve(param_values):
  train_scores = []
  test_scores = []

  for lr in param_values:
    gbc = GradientBoostingClassifier(learning_rate=lr)
    gbc.fit(x_train, y_train)

    # eval on train
    y_pred_train_proba = gbc.predict_proba(x_train)
    train_auc = roc_auc_score(y_train, y_pred_train_proba[:,1])
    train_scores.append(train_auc)

    # eval on test
    y_pred_proba = gbc.predict_proba(x_test)
    test_auc = roc_auc_score(y_test, y_pred_proba[:,1])
    test_scores.append(test_auc)

    print('param value: ' + str(lr) + '; train : ' + str(train_auc) + '; test : ' + str(test_auc))

  plt.plot(param_values, train_scores, label='Train')
  plt.plot(param_values, test_scores, label = 'Test')
  plt.xlabel('Learning Rate')
  plt.ylabel('AUC')
  plt.title('Learning Curve - Hyperparameter Learning Rate - Gradient Boosting')
  plt.legend()
  plt.show()

param_values = [float(x) for x in np.linspace(0.01, 0.03, 20)]
draw_learning_curve(param_values)

#### n_estimators

def draw_learning_curve(param_values):
  train_scores = []
  test_scores = []

  for es in param_values:
    gbc = GradientBoostingClassifier(learning_rate = 0.018571428571428572, n_estimators=es)
    gbc.fit(x_train, y_train)

    # eval on train
    y_pred_train_proba = gbc.predict_proba(x_train)
    train_auc = roc_auc_score(y_train, y_pred_train_proba[:,1])
    train_scores.append(train_auc)

    # eval on test
    y_pred_proba = gbc.predict_proba(x_test)
    test_auc = roc_auc_score(y_test, y_pred_proba[:,1])
    test_scores.append(test_auc)

    print('param value: ' + str(es) + '; train : ' + str(train_auc) + '; test : ' + str(test_auc))

  plt.plot(param_values, train_scores, label='Train')
  plt.plot(param_values, test_scores, label = 'Test')
  plt.xlabel('n_estimators')
  plt.ylabel('AUC')
  plt.title('Learning Curve - Hyperparameter n_estimators - Gradient Boosting')
  plt.legend()
  plt.show()

param_values = [int(x) for x in np.linspace(1, 70, 35)]
draw_learning_curve(param_values)

#### max_depth

def draw_learning_curve(param_values):
  train_scores = []
  test_scores = []

  for md in param_values:
    gbc = GradientBoostingClassifier(learning_rate = 0.018571428571428572, n_estimators=50,
                                     max_depth=md)
    gbc.fit(x_train, y_train)

    # eval on train
    y_pred_train_proba = gbc.predict_proba(x_train)
    train_auc = roc_auc_score(y_train, y_pred_train_proba[:,1])
    train_scores.append(train_auc)

    # eval on test
    y_pred_proba = gbc.predict_proba(x_test)
    test_auc = roc_auc_score(y_test, y_pred_proba[:,1])
    test_scores.append(test_auc)

    print('param value: ' + str(md) + '; train : ' + str(train_auc) + '; test : ' + str(test_auc))

  plt.plot(param_values, train_scores, label='Train')
  plt.plot(param_values, test_scores, label = 'Test')
  plt.xlabel('max_depth')
  plt.ylabel('AUC')
  plt.title('Learning Curve - Hyperparameter max_depth - Gradient Boosting')
  plt.legend()
  plt.show()

param_values = [int(x) for x in np.linspace(1, 10, 15)]
draw_learning_curve(param_values)

#### max_leaf_nodes

def draw_learning_curve(param_values):
  train_scores = []
  test_scores = []

  for md in param_values:
    gbc = GradientBoostingClassifier(learning_rate = 0.018571428571428572, n_estimators=50,
                                     max_depth=4, max_leaf_nodes=md)
    gbc.fit(x_train, y_train)

    # eval on train
    y_pred_train_proba = gbc.predict_proba(x_train)
    train_auc = roc_auc_score(y_train, y_pred_train_proba[:,1])
    train_scores.append(train_auc)

    # eval on test
    y_pred_proba = gbc.predict_proba(x_test)
    test_auc = roc_auc_score(y_test, y_pred_proba[:,1])
    test_scores.append(test_auc)

    print('param value: ' + str(md) + '; train : ' + str(train_auc) + '; test : ' + str(test_auc))

  plt.plot(param_values, train_scores, label='Train')
  plt.plot(param_values, test_scores, label = 'Test')
  plt.xlabel('max_leaf_nodes')
  plt.ylabel('AUC')
  plt.title('Learning Curve - Hyperparameter max_leaf_nodes - Gradient Boosting')
  plt.legend()
  plt.show()

param_values = [int(x) for x in np.linspace(2, 10, 10)]
draw_learning_curve(param_values)

show_best_hyperparameter(gb_tune)

gb_tuned = GradientBoostingClassifier(learning_rate = 0.018571428571428572, n_estimators=50,min_samples_split=3,
                                      min_samples_leaf=5, max_leaf_nodes=9,max_depth=3, n_iter_no_change=2,
                                      random_state=17, warm_start=False)
gb_tuned.fit(x_train, y_train)
eval_classification(gb_tuned)

# Assuming X_test is your test dataset and model is your Gradient Boosting model
y_pred = gb_tuned.predict(x_test)

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, y_pred)

# Calculate the percentage of each category
percentage_matrix = cnf_matrix / cnf_matrix.sum()

# Define the labels
labels = np.array([['True Negative', 'False Positive'],
                   ['False Negative', 'True Positive']])

# Create labels with both count and percentage
new_labels = (np.asarray(["{0}\n{1} ({2:.2%})".format(label, value, percentage)
                          for label, value, percentage in zip(labels.flatten(), cnf_matrix.flatten(), percentage_matrix.flatten())])
             ).reshape(2,2)

# Plot confusion matrix using a heatmap
fig = plt.figure(figsize=(10,7))
fig.patch.set_facecolor('white')  # Change figure color

sns.heatmap(cnf_matrix, annot=new_labels, fmt='', cmap='viridis', annot_kws={"weight": "bold"})  # Change heatmap color to 'viridis'
plt.xlabel('Predicted')
plt.ylabel('Actual')

# Change label colors
plt.gca().xaxis.label.set_color('black')
plt.gca().yaxis.label.set_color('black')

plt.show()

# Assuming X_test is your test dataset and model is your Gradient Boosting model
y_pred = gb_tuned.predict(x_test)

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, y_pred)

# Calculate the percentage of each category
percentage_matrix = cnf_matrix / cnf_matrix.sum()

# Define the labels
labels = np.array([['True Negative', 'False Positive'],
                   ['False Negative', 'True Positive']])

# Create labels with both count and percentage
new_labels = (np.asarray(["{0}\n{1} ({2:.2%})".format(label, value, percentage)
                          for label, value, percentage in zip(labels.flatten(), cnf_matrix.flatten(), percentage_matrix.flatten())])
             ).reshape(2,2)

# Plot confusion matrix using a heatmap
fig = plt.figure(figsize=(10,7))
fig.patch.set_facecolor('#061E45')  # Change figure color

sns.heatmap(cnf_matrix, annot=new_labels, fmt='', cmap='viridis', annot_kws={"weight": "bold"})  # Change heatmap color to 'viridis'
plt.xlabel('Predicted')
plt.ylabel('Actual')

# Change label colors
plt.gca().xaxis.label.set_color('white')
plt.gca().yaxis.label.set_color('white')

plt.show()

### Feature Importance

# Buat objek explainer menggunakan TreeExplainer
explainer = shap.TreeExplainer(gb_tuned)

# Hitung SHAP values untuk data train dan test
shap_values_train = explainer.shap_values(x_train)
shap_values_test = explainer.shap_values(x_test)

# Tampilkan summary plot untuk data train
shap.summary_plot(shap_values_train, x_train, plot_type="bar")

# Tampilkan force plot untuk satu sampel pada data test
shap.force_plot(explainer.expected_value, shap_values_test[0], x_test.iloc[0, :])

# Mengeluarkan feature importance
feature_importance = gb_tuned.feature_importances_

# Menyusun hasil feature importance ke dalam DataFrame (opsional)
feature_importance_df = pd.DataFrame({
    'Feature': x_train.columns,
    'Importance': feature_importance
})

# Menampilkan feature importance secara terurut menurun
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
print(feature_importance_df)

feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=True)
# Membuat bar plot
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.title('Feature Importance')

plt.grid(False)
plt.show()

# Melakukan pengurutan feature importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=True)

# Membuat bar plot
fig = plt.figure(figsize=(11, 7))
fig.patch.set_facecolor('#061E45')
bars = plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='#1ECBE1')
plt.xlabel('Importance', color='white')
plt.title('Feature Importance', color='white', fontsize=20, fontweight='bold')

# Adjust xtick and ytick to white
plt.xticks(color='white')
plt.yticks(color='white')

# Menghilangkan grid
plt.grid(False)

plt.show()


### Business Insight dan Recommendation
akan di jelaskan pada materi ppt dan README Repository
